import streamlit as st
import requests
import datetime
import os
import google.generativeai as genai
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import json

# --- Streamlitã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ©Ÿèƒ½ã‹ã‚‰æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€ ---
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlitã®Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- Googleèªè¨¼æƒ…å ±ã®æº–å‚™ (Workload Identityé€£æº) ---
def get_google_credentials():
    # Streamlit Cloudä¸Šã§GitHub ActionsçµŒç”±ã§å®Ÿè¡Œã•ã‚Œã‚‹å ´åˆã€
    # google-github-actions/auth@v2 ãŒè¨­å®šã™ã‚‹ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•ã§èªè¨¼æƒ…å ±ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã€‚
    # ã“ã®é–¢æ•°ã¯ã€ãã®èªè¨¼æƒ…å ±ã«Google Drive APIã®æ“ä½œç¯„å›²(scope)ã‚’ä»˜ä¸ã™ã‚‹å½¹å‰²ã‚’æŒã¤ã€‚
    try:
        # ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€æ¨™æº–çš„ãªç’°å¢ƒå¤‰æ•°ã‚’æ¢ã—ã¦è‡ªå‹•ã§èªè¨¼ã‚’è©¦ã¿ã‚‹
        creds, project_id = google.auth.default(scopes=['https://www.googleapis.com/auth/drive'])
        return creds
    except Exception as e:
        st.error(f"Google Cloudã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ã“ã“ã‹ã‚‰ä¸‹ã®é–¢æ•°ç¾¤ã¯ã€ä¸€åˆ‡ã®å¤‰æ›´ãªã— ---
def to_wareki_jp(y, m):
    try: y, m = int(y), int(m)
    except (ValueError, TypeError): return f"{y}å¹´{m}æœˆ"
    if y >= 2019: era, era_year = "ä»¤å’Œ", y - 2018
    elif y >= 1989: era, era_year = "å¹³æˆ", y - 1988
    elif y >= 1926: era, era_year = "æ˜­å’Œ", y - 1925
    else: return f"{y}å¹´{m}æœˆ"
    year_str = "å…ƒå¹´" if era_year == 1 else str(era_year)
    return f"{era}{year_str}å¹´{m}æœˆ"

def translate_jp_to_en_for_search(jp_term, model):
    if not model: return ""
    prompt = f"ä»¥ä¸‹ã®æ—¥æœ¬ã®ç—…åã‚’ã€PubMedã§è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã«æœ€ã‚‚é©ã—ãŸè‹±èªã®åŒ»å­¦ç”¨èªã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚\nä½™è¨ˆãªè§£èª¬ã‚„æ–‡ç« ã¯ä¸€åˆ‡å«ã‚ãšã€è‹±èªã®åŒ»å­¦ç”¨èªã®ã¿ã‚’è¿”ç­”ã—ã¦ãã ã•ã„ã€‚\n\n[æ—¥æœ¬ã®ç—…å]\n{jp_term}\n\n[è‹±èªã®åŒ»å­¦ç”¨èª]"
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception as e: st.warning(f"'{jp_term}'ã®è‹±èªã¸ã®å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); return ""

def analyze_and_explain_paper(paper_info, model):
    if not model: return "ï¼ˆåˆ†æãƒ»è§£èª¬ã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰"
    prompt = f"""
ã‚ãªãŸã¯ã€æœ€æ–°ã®åŒ»å­¦è«–æ–‡ã®è¦ç‚¹ã‚’ã€æ—¥æœ¬ã®å¤šå¿™ãªåŒ»å¸«å‘ã‘ã«åˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã™ã‚‹ã€éå¸¸ã«å„ªç§€ãªã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®è«–æ–‡æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€ç™ºè¡¨æ—¥ã€è¦æ—¨ï¼‰ã‚’å…ƒã«ã€ä¸‹è¨˜ã®ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘ã«å³å¯†ã«å¾“ã£ã¦ã€æ—¥æœ¬èªã§è§£èª¬æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
çµ¶å¯¾ã«ã€å…ƒã®è‹±èªã®ã‚¿ã‚¤ãƒˆãƒ«ã‚„è¦æ—¨ã¯å‡ºåŠ›ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚
---
[è«–æ–‡æƒ…å ±]
- ã‚¿ã‚¤ãƒˆãƒ«: {paper_info['title']}
- è‘—è€…: {paper_info['authors']}
- ç™ºè¡¨æ—¥: {paper_info['pub_date_jp']}
- è¦æ—¨: {paper_info['abstract']}
---
ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
â–  ã‚¿ã‚¤ãƒˆãƒ«: ï¼ˆã“ã“ã«æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸã‚¿ã‚¤ãƒˆãƒ«ï¼‰
â–  ç™ºè¡¨æ—¥: {paper_info['pub_date_jp']}
â–  è«–æ–‡ã®çµè«–: ï¼ˆã“ã®è«–æ–‡ãŒæœ€çµ‚çš„ã«ä½•ã‚’è¨€ã£ã¦ã„ã‚‹ã®ã‹ã€æœ€ã‚‚é‡è¦ãªçµè«–ã‚’1ã€œ2è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„ï¼‰
â–  å®Ÿé¨“ã®æ¦‚è¦: ï¼ˆã€Œèª°/ä½•ã‚’å¯¾è±¡ã«ã€ã€Œä½•ã‚’ã—ã¦ã€ã€Œä½•ã‚’èª¿ã¹ãŸã®ã‹ã€ã‚’å…·ä½“çš„ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œã€‡ã€‡ãƒã‚¦ã‚¹ã‚’ç”¨ã„ã¦ã€â–³â–³ãƒ¯ã‚¯ãƒãƒ³ã‚’æ¥ç¨®ã—ã€Ã—Ã—ã«å¯¾ã™ã‚‹æŠ—ä½“ä¾¡ã®å¤‰åŒ–ã‚’æ¸¬å®šã—ãŸã€ï¼‰
â–  çµæœã¨è€ƒå¯Ÿ: ï¼ˆå®Ÿé¨“ã®çµæœã€ä½•ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã®ã‹ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ãã®çµæœãŒè‡¨åºŠçš„ã«ã©ã®ã‚ˆã†ãªæ„å‘³ã‚’æŒã¤ã‹ã€ä»Šå¾Œã®å±•æœ›ãªã©ã‚‚å«ã‚ã¦è§£èª¬ã—ã¦ãã ã•ã„ï¼‰
"""
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception as e: st.warning(f"Gemini APIã§ã®è«–æ–‡è§£èª¬ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); return "ï¼ˆè«–æ–‡ã®è§£èª¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼‰"

def search_pubmed(english_term, days=30):
    base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"    
    end_date = datetime.date.today(); start_date = end_date - datetime.timedelta(days=days)
    date_query = f"{start_date.strftime('%Y/%m/%d')}[PDAT] : {end_date.strftime('%Y/%m/%d')}[PDAT]"
    search_url = f"{base_url}esearch.fcgi"; search_params = {"db": "pubmed", "term": f"({english_term}[MeSH Terms] OR {english_term}[Title/Abstract]) AND ({date_query})", "retmode": "json", "retmax": 5}
    try:
        response = requests.get(search_url, params=search_params); response.raise_for_status(); data = response.json()
        id_list = data.get("esearchresult", {}).get("idlist", [])        
        if not id_list: return []
        fetch_url = f"{base_url}efetch.fcgi"; fetch_params = {"db": "pubmed", "id": ",".join(id_list), "retmode": "xml"}        
        response = requests.get(fetch_url, params=fetch_params); response.raise_for_status()
        from xml.etree import ElementTree as ET
        root = ET.fromstring(response.content); articles = []
        month_map = {'Jan': '1', 'Feb': '2', 'Mar': '3', 'Apr': '4', 'May': '5', 'Jun': '6', 'Jul': '7', 'Aug': '8', 'Sep': '9', 'Oct': '10', 'Nov': '11', 'Dec': '12'}
        for article in root.findall(".//PubmedArticle"):
            abstract_text = article.findtext(".//AbstractText")
            if not abstract_text: continue
            pub_date_node = article.find(".//PubDate")
            if pub_date_node is not None:
                year = pub_date_node.findtext("Year", "ä¸æ˜"); month_str = pub_date_node.findtext("Month", "ä¸æ˜"); month = month_map.get(month_str, month_str) 
                pub_date_jp_str = to_wareki_jp(year, month)
            else: pub_date_jp_str = "ç™ºè¡¨æ—¥ä¸æ˜"
            articles.append({"title": article.findtext(".//ArticleTitle") or "N/A", "authors": ", ".join([f"{a.findtext('LastName')} {a.findtext('Initials')}" for a in article.findall('.//Author') if a.findtext('LastName')]) or "N/A", "abstract": abstract_text, "url": f"https://pubmed.ncbi.nlm.nih.gov/{article.findtext('.//PMID')}/", "pub_date_jp": pub_date_jp_str})
        return articles
    except requests.RequestException as e: st.warning(f"PubMed APIã§ã‚¨ãƒ©ãƒ¼: {e}"); return []

def create_google_doc(title, content, creds, folder_id):
    try:
        # Google Docs APIã¨Drive APIã®ä¸¡æ–¹ã‚’ä½¿ã†
        drive_service = build('drive', 'v3', credentials=creds)
        docs_service = build('docs', 'v1', credentials=creds)
        
        # ã¾ãšDriveã«ç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
        file_metadata = {'name': title, 'mimeType': 'application/vnd.google-apps.document', 'parents': [folder_id]}
        document = drive_service.files().create(body=file_metadata).execute()
        doc_id = document.get('id')
        doc_url = f"https://docs.google.com/document/d/{doc_id}/edit"
        
        # æ¬¡ã«Docs APIã§å†…å®¹ã‚’æ›¸ãè¾¼ã‚€
        requests_body = [{'insertText': {'location': {'index': 1}, 'text': content}}]
        docs_service.documents().batchUpdate(documentId=doc_id, body={'requests': requests_body}).execute()
        return doc_url
    except HttpError as err: st.error(f"Google Drive/Docs APIã§ã‚¨ãƒ©ãƒ¼: {err}"); return None

# --- Streamlitã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ---
st.set_page_config(page_title="æœ€æ–°åŒ»å­¦è«–æ–‡ãŠã¾ã‹ã›ã‚µãƒãƒªãƒ¼", layout="centered")
st.title("ğŸ‘¨â€âš•ï¸ æœ€æ–°åŒ»å­¦è«–æ–‡ãŠã¾ã‹ã›ã‚µãƒãƒªãƒ¼")
st.markdown("çŸ¥ã‚ŠãŸã„ç—…åã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ—¥æœ¬èªã§å…¥åŠ›ã™ã‚‹ã¨ã€AIãŒæµ·å¤–ã®æœ€æ–°è«–æ–‡ã‚’æ¤œç´¢ãƒ»åˆ†æã—ã€è¦ç‚¹è§£èª¬ãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ã§Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ä½œæˆã—ã¾ã™ã€‚")

# â˜…ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã™ã‚‹Google Driveãƒ•ã‚©ãƒ«ãƒ€ã®IDã‚’å…¥åŠ›ã•ã›ã‚‹
DRIVE_FOLDER_ID = st.text_input("ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã™ã‚‹Google Driveãƒ•ã‚©ãƒ«ãƒ€ã®IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", help="Googleãƒ‰ãƒ©ã‚¤ãƒ–ã§ã€ã“ã®ã‚¢ãƒ—ãƒªå°‚ç”¨ã«ä½œæˆãƒ»å…±æœ‰è¨­å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ãã€URLã®æœ€å¾Œã®éƒ¨åˆ†ã«ã‚ã‚‹è‹±æ•°å­—ã®ç¾…åˆ—ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")

with st.form("search_form"):
    jp_disease_input = st.text_input("ã“ã“ã«ç—…åã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ç³–å°¿ç—…, é«˜è¡€åœ§ï¼‰", "")
    submitted = st.form_submit_button("ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚’é–‹å§‹")

if submitted and jp_disease_input and DRIVE_FOLDER_ID:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    creds = get_google_credentials()
    if not creds:
        st.stop()

    jp_disease_list = [name.strip() for name in jp_disease_input.split(',') if name.strip()]
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    st.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚å®Œäº†ã¾ã§æ•°åˆ†ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™â€¦")

    for jp_disease in jp_disease_list:
        with st.status(f"ã€{jp_disease}ã€‘ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­â€¦", expanded=True) as status:
            st.write("1. è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ä¸­...")
            english_term = translate_jp_to_en_for_search(jp_disease, model)
            if not english_term: st.warning(f"å¤‰æ›å¤±æ•—ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"); status.update(label="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ", state="error"); continue
            st.write(f"-> `{english_term}`")
            
            st.write("2. PubMedã§è«–æ–‡ã‚’æ¤œç´¢ä¸­...")
            papers = search_pubmed(english_term)
            if not papers: st.warning(f"è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); status.update(label="è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", state="warning"); continue
            st.write(f"-> {len(papers)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹")

            st.write("3. AIãŒåˆ†æãƒ»è§£èª¬ã‚’ä½œæˆä¸­â€¦")
            final_content = f"ã€{jp_disease}ã€‘ã«é–¢ã™ã‚‹æœ€æ–°è«–æ–‡è§£èª¬ãƒ¬ãƒãƒ¼ãƒˆ\nä½œæˆæ—¥: {today_str}\n\n========================================\n\n"
            for j, paper in enumerate(papers):
                st.write(f"   - è«–æ–‡ {j+1}/{len(papers)} ã‚’å‡¦ç†ä¸­...")
                explanation = analyze_and_explain_paper(paper, model)
                final_content += f"ã€è«–æ–‡ {j+1}ã€‘\n{explanation}\n\nâ–  å‚ç…§å…ƒURL: {paper['url']}\n\n----------------------------------------\n\n"
            
            st.write("4. Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­...")
            doc_title = f"{jp_disease} æœ€æ–°è«–æ–‡è§£èª¬ãƒ¬ãƒãƒ¼ãƒˆ ({today_str})"
            doc_url = create_google_doc(doc_title, final_content, creds, DRIVE_FOLDER_ID)
            if doc_url:
                st.success(f"ã€Œ{jp_disease}ã€ã®ãƒ¬ãƒãƒ¼ãƒˆãŒå®Œæˆã—ã¾ã—ãŸï¼")
                st.markdown(f"**[å®Œæˆã—ãŸãƒ¬ãƒãƒ¼ãƒˆã‚’é–‹ã]({doc_url})**", unsafe_allow_html=True)
                status.update(label="ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†ï¼", state="complete")
            else:
                status.update(label="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆã«å¤±æ•—", state="error")

    st.success("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
